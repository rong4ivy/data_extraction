{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_VirtualList' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/rongwang/Documents/Dekra/data extraction.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Replace 'your_pdf_file.pdf' with the path to your PDF file\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m pdf_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m6131666.pdf\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m result_df \u001b[39m=\u001b[39m extract_data_from_pdf(pdf_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Print the resulting DataFrame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(result_df)\n",
      "\u001b[1;32m/Users/rongwang/Documents/Dekra/data extraction.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mReportName\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mReportNumber\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mMission\u001b[39m\u001b[39m'\u001b[39m: [],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mAuditDate\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mReportDate\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mInspector\u001b[39m\u001b[39m'\u001b[39m: [],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEquipmentUser\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mMachine\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mDateOfPlacingInMarket_New\u001b[39m\u001b[39m'\u001b[39m: [],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mDateOfPlacingInEstablishment\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mProducer\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mCountryOfProducer\u001b[39m\u001b[39m'\u001b[39m: [],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEnsemblier\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mPhaseRapport\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mTextFindings\u001b[39m\u001b[39m'\u001b[39m: []}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m page_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_pages):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     page \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39mpages(page_num)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     text \u001b[39m=\u001b[39m page\u001b[39m.\u001b[39mextractText()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rongwang/Documents/Dekra/data%20extraction.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# Use regular expressions to extract relevant information\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: '_VirtualList' object is not callable"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "        \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages[0])\n",
    "\n",
    "        data = {'ReportName': [], 'ReportNumber': [], 'Mission': [],\n",
    "                'AuditDate': [], 'ReportDate': [], 'Inspector': [],\n",
    "                'EquipmentUser': [], 'Machine': [], 'DateOfPlacingInMarket_New': [],\n",
    "                'DateOfPlacingInEstablishment': [], 'Producer': [], 'CountryOfProducer': [],\n",
    "                'Ensemblier': [], 'PhaseRapport': [], 'TextFindings': []}\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages(page_num)\n",
    "            text = page.extractText()\n",
    "\n",
    "            # Use regular expressions to extract relevant information\n",
    "            report_info = re.search(r'Report[^\\n]*', text)\n",
    "            if report_info:\n",
    "                data['ReportName'].append(report_info.group())\n",
    "\n",
    "            report_number_info = re.search(r'Report number[^\\n]*', text)\n",
    "            if report_number_info:\n",
    "                data['ReportNumber'].append(report_number_info.group())\n",
    "\n",
    "            mission_info = re.search(r'Mission[^\\n]*', text)\n",
    "            if mission_info:\n",
    "                data['Mission'].append(mission_info.group())\n",
    "\n",
    "            # Continue extracting other fields using similar patterns\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "# Replace 'your_pdf_file.pdf' with the path to your PDF file\n",
    "pdf_path = '6131666.pdf'\n",
    "result_df = extract_data_from_pdf(pdf_path)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " worl\n",
      "world\n",
      "Welcome\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello world \\nWelcome to regex world\"\n",
    "\n",
    "# Match at the beginning of the string\n",
    "match_beginning = re.search(r'^Hello([^\\n]{5})', text)\n",
    "\n",
    "\n",
    "print(match_beginning.group(1) if match_beginning else \"No match\")\n",
    "\n",
    "# Match at the end of the string\n",
    "match_end = re.search(r'world$', text)\n",
    "print(match_end.group() if match_end else \"No match\")\n",
    "\n",
    "# Multiline mode: match at the beginning of a line\n",
    "multiline_match_beginning = re.search(r'^Welcome', text, re.MULTILINE)\n",
    "print(multiline_match_beginning.group() if multiline_match_beginning else \"No match\")\n",
    "\n",
    "# Multiline mode: match at the end of a line\n",
    "multiline_match_end = re.search(r'world$', text, re.MULTILINE)\n",
    "print(multiline_match_end.group() if multiline_match_end else \"No match\")\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: Hello world, welcome to\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello world, welcome to regex world.\"\n",
    "# Match \"Hello\" followed by exactly 3 words\n",
    "pattern = r'Hello\\b[\\s,]+\\b\\w+\\b(?:[\\s,]+\\b\\w+\\b){2}'\n",
    "\n",
    "match = re.search(pattern, text)\n",
    "if match:\n",
    "    print(f\"Matched: {match.group()}\")\n",
    "else:\n",
    "    print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification report\n",
      "Registration number.: 6131666 Ind.0\n",
      "Compliance of work equipment excluding lifting\n",
      "equipment\n",
      "Shanghai Valeo Automotive Electrical\n",
      "Systems Co., Ltd Verified\n",
      "Company IBSG-8 Rotor Assembly Line\n",
      "No. 5101, Huadong Rd, Pudong, Shanghai, installation\n",
      "China\n",
      "Wujiang Jinlan Machinery Manufacture Co.\n",
      "Shanghai Valeo Automotive Electrical Place of Ltd.\n",
      "Systems Co., Ltd verification 399 Ludang Road, Wujiang district, Suzhou\n",
      "No. 5101 Huadong Road, Pudong, city, Jiangsu, China\n",
      "Invoicing address\n",
      "Shanghai\n",
      "201201 Shanghai\n",
      "CHINA Nature of the Documentary assessment of work\n",
      "audit equipment\n",
      "Dates 27/05/2022\n",
      "Intervenant(s)\n",
      "Representative of\n",
      "Mr. Guangming Ye DEKRA Mr. Sparky Sun\n",
      "the company\n",
      "DTC shanghai\n",
      "Customer\n",
      "-\n",
      "reference\n",
      "Attached\n",
      "Nothing\n",
      "document\n",
      "Number of copies This report was published on 30/05/2022\n",
      "Page 1/45\n",
      "DEKRA Testing and Certification (Shanghai) Ltd.\n",
      "3F, #250 Jiangchangsan Road, Building 16, Headquarter Economy Park Shibei Hi-Tech Park, Jing’an District, Shanghai, 200436, China\n",
      "T +86 21 6056 7666 F +86 21 6056 7555 www.dekra-certification.com\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "pdf_path = '6131666.pdf'\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "        summary_page = pdf.pages[0]  # Assuming the table of contents is on the first page\n",
    "        text = summary_page.extract_text()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Page 1/45  \n",
      "DEKRA Testing and Certification (Shanghai) Ltd.   \n",
      "3F, #250 Jiangchangsan Road, Building 16, Headquarter Economy Park Shibei Hi -Tech Park, Jing ’an District, Shanghai, 200436, China  \n",
      "T +86 21 6056 7666    F  +86 21 6056 7555    www.dekra -certification.com  \n",
      "   Verification report  \n",
      "  Registration number .: 6131666  Ind.0  \n",
      " \n",
      " \n",
      "  \n",
      " Compliance of work equipment excluding lifting \n",
      "equipment  \n",
      " \n",
      "Company  Shanghai Valeo Automotive Electrical \n",
      "Systems Co., Ltd  \n",
      "No. 5101, Huadong Rd, Pudong, Shanghai, \n",
      "China  Verified \n",
      "installation  IBSG -8 Rotor Assembly Line  \n",
      " \n",
      "Invoicing address  Shanghai Valeo Automotive Electrical \n",
      "Systems Co., Ltd  \n",
      "No. 5101 Huadong Road, Pudong, \n",
      "Shanghai  \n",
      "201201 Shanghai  \n",
      "CHINA  Place of \n",
      "verification  Wujiang Jinlan  Machinery Manufacture Co. \n",
      "Ltd. \n",
      "399 Ludang Road, Wujiang district, Suzhou \n",
      "city, Jiangsu, China  \n",
      "  \n",
      "Nature of the \n",
      "audit  Documentary  assessment of work \n",
      "equipment  \n",
      "    \n",
      "  Dates  27/05/2022 \n",
      " \n",
      "Representative of \n",
      "the company  Mr. Guangming Ye  Intervenant(s) \n",
      "DEKRA     \n",
      "DTC shanghai  Mr. Sparky Sun  \n",
      "    \n",
      "Customer \n",
      "reference  -  \n",
      " \n",
      "Attached \n",
      "document  Nothing    \n",
      " \n",
      "Number of copies  This report was published on 30/05/2022   \n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "pdf_path = '6131666.pdf'\n",
    "with open(pdf_path, 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Get the first page\n",
    "    \n",
    "    pages = reader.pages[0]\n",
    "    text = pages.extract_text()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Clean up multiple spaces while preserving single newlines\n",
    "    #text = re.sub(r'[ \\t]+', ' ', text)  # Replace multiple spaces/tabs with a single space\n",
    "    #text = re.sub(r'\\n(?=[a-zA-Z])', ' ', text)\n",
    "\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nature of the  audit Documentary assessment of work  equipment \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace line breaks followed by a lowercase letter or certain characters with a space\n",
    "    cleaned_text = re.sub(r'\\n(?=[a-z\\-])', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage with your extracted text\n",
    "extracted_text = \"\"\"\n",
    "Nature of the \n",
    "audit Documentary assessment of work \n",
    "equipment \n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = clean_text(extracted_text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/rongwang/miniconda3/envs/nlp/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/rongwang/miniconda3/envs/nlp/lib/python3.11/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rongwang/miniconda3/envs/nlp/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rongwang/miniconda3/envs/nlp/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/rongwang/miniconda3/envs/nlp/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rongwang/miniconda3/envs/nlp/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m748.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-4.9.3-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Downloading lxml-4.9.3-cp311-cp311-macosx_11_0_universal2.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m603.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-4.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XRef object at 2653695 can not be read, some object may be missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XRef object at 1764852 can not be read, some object may be missing\n",
      "XRef object at 3899290 can not be read, some object may be missing\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "XRef object at 1669748 can not be read, some object may be missing\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['FileName', 'RegistrationNumber', 'VerifiedInstallation',\n",
      "       'PlaceVerification', 'NatureAudit', 'DateInspection', 'Intervenant',\n",
      "       'DateReport', 'PhaseReport', 'Producer', 'UserOfMachine',\n",
      "       'DateInMarketNewCondtion', 'DateInMarketEstablishment', 'TextFindings'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_summary(pdf_path):\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        summary_page = pdf.pages[2]  # Assuming the table of contents is on the fourth page\n",
    "        text = summary_page.extract_text()\n",
    "\n",
    "        # Initialize variables\n",
    "        start_of_section = None\n",
    "        end_of_section = None\n",
    "        \n",
    "        # Search for the start of the section\n",
    "        start_of_section_match = re.search(r\"\\s*reserves?\\s+(?:and|&)\\s+proposals?\\s*:?\\.*\\s+(\\d+)\", text, re.I)\n",
    "        if start_of_section_match:\n",
    "            start_of_section = int(start_of_section_match.group(1).strip())\n",
    "\n",
    "        # Search for the end of the section\n",
    "        end_of_section_match = re.search(r\"\\s*Table\\s*of\\s*conformity\\s*\\.*\\s+(\\d+)\", text)\n",
    "        if end_of_section_match:\n",
    "            end_of_section = int(end_of_section_match.group(1).strip())\n",
    "\n",
    "        combined_text = ''\n",
    "\n",
    "        for i in range(start_of_section, end_of_section): # Adjust page range as needed\n",
    "            page = pdf.pages[i]\n",
    "            table = page.extract_table()\n",
    "\n",
    "            if table is not None:\n",
    "                 df = pd.DataFrame(table[1:], columns=table[0])  # Assuming the first row is the header\n",
    "\n",
    "                 if df.shape[1] >= 2:  # Check if there are at least two columns\n",
    "                # Extract the second column and concatenate its text\n",
    "                     combined_text += ';\\n '.join(df.iloc[:, 1].dropna().astype(str).apply(lambda text: re.sub(r'[^\\x00-\\x7F]+', '', text))) + '\\n'\n",
    "                # filter out the Chinese words \n",
    "    return combined_text\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    \n",
    "    # Initialize variables to store extracted information\n",
    "    registration_number = None\n",
    "    verified_installation = None\n",
    "    place_of_verification = None\n",
    "    nature_of_audit = None\n",
    "    nature_of_audit = None\n",
    "    dates = None\n",
    "    intervenants = None\n",
    "    published_on = None\n",
    "    producer = None\n",
    "    user_of_machine= None\n",
    "    date_of_placing_condition= None\n",
    "    date_of_placing_establishment=None\n",
    "    phase_report=None\n",
    "    file_name= None\n",
    "    \n",
    "    file_name_match = re.search(r\"[^/\\\\]+$\", pdf_path)\n",
    "    if file_name_match:\n",
    "        file_name = file_name_match.group()\n",
    "    \n",
    "    phase_report_match = re.search(r\"(FAT|SAT)\", pdf_path)\n",
    "    if phase_report_match :\n",
    "        phase_report = phase_report_match.group().strip()\n",
    "    \n",
    "    \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "            \n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            first_page = reader.pages[0]\n",
    "            text0 = first_page.extract_text()\n",
    "             # combine the lines but keep mutiple newlines\n",
    "            text0 = re.sub(r'\\n(?=[a-zA-Z])', ' ', text0)\n",
    "            \n",
    "            third_page = reader.pages[3]\n",
    "            text3 = third_page.extract_text()\n",
    "        \n",
    "            # Extracting each piece of information using regular expressions\n",
    "            registration_number_match = re.search(r\"\\s*number([^\\n]*)\", text0, re.I)\n",
    "            if registration_number_match:\n",
    "                registration_number = registration_number_match.group(1).strip()\n",
    "                registration_number = registration_number.lstrip(':.').strip()\n",
    "\n",
    "            verified_installation_match = re.search(r\"Verified\\s+installation\\s+([^\\n]+)\", text0)\n",
    "            if verified_installation_match:\n",
    "                verified_installation = verified_installation_match.group(1).strip()\n",
    "\n",
    "            place_of_verification_match = re.search(r\"Place\\s+of\\s+verification\\s*(.*?)(?:s*\\.|$)\", text0)\n",
    "            #place_of_verification_match = re.search(r\"Place\\s+of\\s+verification\\s+([\\s\\S]*?Ltd\\.)\", text0)\n",
    "            if place_of_verification_match:\n",
    "                place_of_verification = place_of_verification_match.group(1).strip()\n",
    "                \n",
    "                \n",
    "            nature_of_audit_match = re.search(r\"\\s*Nature\\s+of\\s+the\\s+audit([^\\n]*)\", text0, re.I)\n",
    "            if nature_of_audit_match:\n",
    "                nature_of_audit = nature_of_audit_match.group(1).strip()\n",
    "                \n",
    "            # Extracting 'dates' \n",
    "            dates_match = re.search(r\"(?:Dates|Audit dates)\\s+([^\\n]*)\", text0)\n",
    "            if dates_match:\n",
    "                dates = dates_match.group(1).strip()\n",
    "\n",
    "            # Extracting 'intervenants' using regular expressions\n",
    "            intervenants_match = re.search(r\"Intervenant([^\\n]*)\", text0)\n",
    "            if intervenants_match:\n",
    "                intervenants_full = intervenants_match.group(1).strip()\n",
    "                intervenants_words = intervenants_full.split()  # Split the string into a list of words\n",
    "                last_three_words = intervenants_words[-3:]  # Get the last three words\n",
    "                intervenants = ' '.join(last_three_words)\n",
    "\n",
    "            # Extracting 'date of report' (published on) using regular expressions\n",
    "            published_on_match = re.search(r\"published on([^\\n]*)\", text0)\n",
    "            if published_on_match:\n",
    "                published_on = published_on_match.group(1).strip()\n",
    "            \n",
    "            \n",
    "            # extract info from page 3\n",
    "            company_match = re.search(r\"\\s+for\\s+placing\\s+on\\s+the\\s+mark\\s*et\\s*([^\\n]*)\", text3)\n",
    "            #company_match = re.search(r\"mark\\s*et\\s*\\n((?:|[^\\n]*)\", text)\n",
    "            if company_match:\n",
    "                producer= company_match.group(1).strip()\n",
    "                \n",
    "            user_of_machine_match =re.search(r\"Place of use ([^\\n]*)\",text3)\n",
    "            if user_of_machine_match:\n",
    "                user_of_machine =user_of_machine_match.group(1).strip()\n",
    "        \n",
    "            date_of_placing_condition_match =re.search(r\"In new condition ([^\\n]*)\",text3)\n",
    "            if date_of_placing_condition_match:\n",
    "                date_of_placing_condition =date_of_placing_condition_match.group(1).strip()\n",
    "            \n",
    "            date_of_placing_establishment_match= re.search(r\"In the establishment([^\\n]*)\",text3)\n",
    "            if date_of_placing_establishment_match:\n",
    "                date_of_placing_establishment = date_of_placing_condition_match.group(1).strip()\n",
    "            \n",
    "                \n",
    "            text_findings= parse_summary(pdf_path)\n",
    "            \n",
    "                \n",
    "        # Creating a DataFrame with the extracted information\n",
    "            data = {\n",
    "            \"FileName\": file_name,\n",
    "            'RegistrationNumber': registration_number,\n",
    "            'VerifiedInstallation': verified_installation,\n",
    "            'PlaceVerification':place_of_verification,\n",
    "            'NatureAudit': nature_of_audit,\n",
    "            'DateInspection': dates,\n",
    "            'Intervenant': intervenants,\n",
    "            'DateReport': published_on,\n",
    "            \"PhaseReport\": phase_report,\n",
    "            'Producer':producer,\n",
    "            \"UserOfMachine\":user_of_machine,\n",
    "            \"DateInMarketNewCondtion\": date_of_placing_condition,\n",
    "            \"DateInMarketEstablishment\": date_of_placing_establishment,\n",
    "            \"TextFindings\": text_findings\n",
    "            \n",
    "            }\n",
    "    return data\n",
    "\n",
    "        \n",
    "def process_directory(directory_path):\n",
    "   \n",
    "    all_data = []  # List to store all data dictionaries\n",
    "\n",
    "    # Process each file in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(directory_path, filename)\n",
    "            data = extract_data_from_pdf(pdf_path)\n",
    "            \n",
    "            # Append the data dictionary to the list\n",
    "            all_data.append(data)\n",
    "            \n",
    "# Each dictionary in the list becomes a row in the DataFrame, and the keys of the dictionaries become the column headers\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df_china = pd.DataFrame(all_data)\n",
    "    \n",
    "    return df_china\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "# Specify the directory path containing the PDF files\n",
    "directory_path = '/Users/rongwang/Documents/Dekra/tracy'\n",
    "\n",
    "df_china =  process_directory(directory_path)\n",
    "print(df_china.columns)\n",
    "\n",
    "#textfindings_df = df_china[\"file_name\",\"phase of report\",\"text findings\"]\n",
    "#textfindings_df.to_json(\"text_findings.json\", orient='records', lines=True)\n",
    "# Assuming df is your DataFrame\n",
    "#textfindings_df.to_xml('text_findings.xml', index=False)\n",
    "\n",
    "#df_china.to_csv(tracy_file,index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "df_china.to_excel('Tracy_5_files.xlsx', index=False)\n",
    "# Now use all_columns as the attr_cols value\n",
    "df_china.to_xml('Tracy_5_files.xml', index=True)\n",
    "df_china.to_csv(\"Tracy_5_files.csv\",index=False)\n",
    "#print(df_china)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df_china is your DataFrame\n",
    "all_columns = df_china.columns.tolist()\n",
    "\n",
    "# Now use all_columns as the attr_cols value\n",
    "df_china.to_xml('tracy_5_files.xml', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a CSV file\n",
    "csv_file_path = 'test_China.csv'  # Specify your desired file path\n",
    "df_china.to_csv(csv_file_path, index=False)  # 'index=False' to exclude row indices from the CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
