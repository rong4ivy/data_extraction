{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def parse_summary(pdf_path):\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        summary_page = pdf.pages[2]  # Assuming the table of contents is on the fourth page\n",
    "        text = summary_page.extract_text()\n",
    "\n",
    "        # Initialize variables\n",
    "        start_of_section = None\n",
    "        end_of_section = None\n",
    "        \n",
    "        # Search for the start of the section\n",
    "        start_of_section_match = re.search(r\"\\s*(Réserves\\s*et\\s*propositions\\s*:?|Liste des observations relevées / commentaires)\\s*\\.*\\s*(\\d+)\", text, re.I)\n",
    "        #start_of_section_match = re.search(r\"\\s*Réserves\\s+et\\s+Propositions\\s*(\\d+)\\s*$\", text, re.I | re.M)\n",
    "\n",
    "        if start_of_section_match:\n",
    "            start_of_section = int(start_of_section_match.group(2).strip())-1\n",
    "\n",
    "        # Search for the end of the section \n",
    "        end_of_section_match = re.search(r\"\\s*(Tableau\\s*de\\s*conformité|Annexe)\\s*\\.*\\s*(\\d+)\", text,re.I)\n",
    "      \n",
    "        if end_of_section_match:\n",
    "            end_of_section = int(end_of_section_match.group(2).strip())-1\n",
    "        else: \n",
    "            end_of_section = len(pdf.pages)-1\n",
    "       \n",
    "        #print(start_of_section, end_of_section)\n",
    "\n",
    "        combined_text = \" \"\n",
    "        for i in range(start_of_section, end_of_section):  # Adjust page range as needed\n",
    "            page = pdf.pages[i]\n",
    "            table = page.extract_table()\n",
    "           # print(table)\n",
    "\n",
    "            if table is not None:\n",
    "                df = pd.DataFrame(table[2:], columns=table[0])  # Assuming the first two rows are headers\n",
    "                #print(df.columns)\n",
    "\n",
    "                for row in df.itertuples(index=False):  # Iterate over rows\n",
    "                    serial_number = row[0]\n",
    "                    split_pattern = r'(Préconisation|Propositions|Proposition)[.,;:\\s]*'\n",
    "                    content = re.split(split_pattern, row[1], maxsplit=1,flags=re.IGNORECASE)[0]\n",
    "                     # Split content at 'Préconisation' or the other words at most once and take the first part\n",
    "\n",
    "                    combined_text += f\"{serial_number}. {content};\\n\"  # Concatenate serial number and content\n",
    "                    \n",
    "    #print(combined_text)\n",
    "                    \n",
    "    return combined_text\n",
    "      \n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    \n",
    "    # Initialize variables to store extracted information\n",
    "    registration_number = None\n",
    "    verified_installation = None\n",
    "    place_of_verification = None\n",
    "    nature_of_audit = None\n",
    "    dates = None\n",
    "    intervenants = None\n",
    "    published_on = None\n",
    "    producer = None\n",
    "    user_of_machine= None\n",
    "    date_of_placing_condition= None\n",
    "    date_of_placing_establishment=None\n",
    "    phase_report=None\n",
    "    file_name= None\n",
    "    \n",
    "    file_name_match = re.search(r\"[^/\\\\]+$\", pdf_path)\n",
    "    if file_name_match:\n",
    "        file_name = file_name_match.group()\n",
    "    \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "            \n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            first_page = reader.pages[0]\n",
    "            text0 = first_page.extract_text()\n",
    "             # combine the lines but keep mutiple newlines\n",
    "            text0 = re.sub(r'\\n(?=[a-zA-Z])', ' ', text0)\n",
    "            \n",
    "            third_page = reader.pages[3]\n",
    "            text3 = third_page.extract_text()          # Extracting each piece of information using regular expressions\n",
    "            \n",
    "            registration_number_match = re.search(r\"E\\d+\\s*[A-Z]?\\d*\\s*(Ind\\.[01])?\", text0, re.I)\n",
    "            if registration_number_match:\n",
    "                registration_number = registration_number_match.group(0).strip()\n",
    "            registration_number_match = re.search(r\"\\s+enregistrement([^\\n]*)\", text0, re.I)\n",
    "            \n",
    "            #if registration_number_match:\n",
    "             #   registration_number = registration_number_match.group(1).strip()\n",
    "              #  registration_number = registration_number.lstrip(':.').strip()\n",
    "\n",
    "            verified_installation_match = re.search(r\"Installation\\s+vérifiée\\s+([^\\n]+)\", text0,re.I)\n",
    "            if verified_installation_match:\n",
    "                verified_installation = verified_installation_match.group(1).strip()\n",
    "\n",
    "            #place_of_verification_match = re.search(r\"Place\\s+of\\s+verification\\s*(.*?)(?:s*\\.|$)\", text0)\n",
    "            #place_of_verification_match = re.search(r\"Place\\s+of\\s+verification\\s+([\\s\\S]*?Ltd\\.)\", text0)\n",
    "            #if place_of_verification_match:\n",
    "             #   place_of_verification = place_of_verification_match.group(1).strip()\n",
    "                \n",
    "                \n",
    "            nature_of_audit_match = re.search(r\"\\s*Nature\\s+de\\s+la\\s+vérification\\s+([^\\n]*)\", text0, re.I)\n",
    "            if nature_of_audit_match:\n",
    "                nature_of_audit = nature_of_audit_match.group(1).strip()\n",
    "                \n",
    "            # Extracting 'dates' \n",
    "            #dates_match = re.search(r\"(?:Dates|Audit dates)\\s+([^\\n]*)\", text0)\n",
    "            dates_match = re.search(r\"\\s*Dates\\s+de\\s+vérification\\s+([^\\n]*)\", text0,re.I)\n",
    "            if dates_match:\n",
    "                dates = dates_match.group(1).strip()\n",
    "\n",
    "            # Extracting 'intervenants' using regular expressions\n",
    "            intervenants_match = re.search(r\"\\s*Intervenant\\(s\\)([^\\n]*)\", text0)\n",
    "            if intervenants_match:\n",
    "                intervenants_full = intervenants_match.group(1).strip()\n",
    "                intervenants_words = intervenants_full.split()  # Split the string into a list of words\n",
    "                last_three_words = intervenants_words[-2:]  # Get the last three words\n",
    "                intervenants = ' '.join(last_three_words)\n",
    "\n",
    "            # Extracting 'date of report' (published on) using regular expressions\n",
    "            published_on_match = re.search(r\"\\s*été\\s+édité\\s*([^\\n]*)\", text0)\n",
    "            if published_on_match:\n",
    "                published_on = published_on_match.group(1).strip()\n",
    "            \n",
    "            \n",
    "            # extract info from page 3\n",
    "            company_match = re.search(r\"Marque([^\\n]*)\", text3,re.I)\n",
    "            #company_match = re.search(r\"mark\\s*et\\s*\\n((?:|[^\\n]*)\", text)\n",
    "            if company_match:\n",
    "                producer= company_match.group(1).strip()\n",
    "                \n",
    "            user_of_machine_match =re.search(r\"Établissement([^\\n]*)\",text3)\n",
    "            if user_of_machine_match:\n",
    "                user_of_machine =user_of_machine_match.group(1).strip()\n",
    "        \n",
    "            date_of_placing_condition_match =re.search(r\"\\s*à\\s+l’état\\s+neuf\\s*([^\\n]*)\",text3)\n",
    "            if date_of_placing_condition_match:\n",
    "                date_of_placing_condition =date_of_placing_condition_match.group(1).strip()\n",
    "            \n",
    "            date_of_placing_establishment_match= re.search(r\"\\s*dans\\s+l’établissement\\s*([^\\n]*)\",text3)\n",
    "            if date_of_placing_establishment_match:\n",
    "                date_of_placing_establishment = date_of_placing_condition_match.group(1).strip()\n",
    "                \n",
    "            phase_report_match= re.search(r\"Cumul.*\\n(\\S+)\\s+(\\S+)\",text3)\n",
    "            if phase_report_match:\n",
    "                phase_report =phase_report_match.group(2).strip()\n",
    "            \n",
    "            text_findings= parse_summary(pdf_path)\n",
    "            \n",
    "                \n",
    "        # Creating a DataFrame with the extracted information\n",
    "            data = {\n",
    "            \"FileName\": file_name,\n",
    "            'RegistrationNumber': registration_number,\n",
    "            'VerifiedInstallation': verified_installation,\n",
    "            'PlaceVerification':place_of_verification,\n",
    "            'NatureAudit': nature_of_audit,\n",
    "            'DateInspection': dates,\n",
    "            'Intervenant': intervenants,\n",
    "            'DateReport': published_on,\n",
    "            \"PhaseReport\": phase_report,\n",
    "            'Producer':producer,\n",
    "            \"UserOfMachine\":user_of_machine,\n",
    "            \"DateInMarketNewCondtion\": date_of_placing_condition,\n",
    "            \"DateInMarketEstablishment\": date_of_placing_establishment,\n",
    "            \"TextFindings\": text_findings\n",
    "            \n",
    "            }\n",
    "    return data\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    all_data = []  # List to store all data dictionaries\n",
    "    failed_files = []  # List to store the names of files that caused errors\n",
    "\n",
    "    # Process each file in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            try:\n",
    "                pdf_path = os.path.join(directory_path, filename)\n",
    "                data = extract_data_from_pdf(pdf_path)\n",
    "                all_data.append(data)  # Append the data dictionary to the list\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "                failed_files.append(filename)  # Store the name of the file that failed\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df_France = pd.DataFrame(all_data)\n",
    "\n",
    "    # Optionally, return the list of failed files as well\n",
    "    return df_France, failed_files\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['FileName', 'RegistrationNumber', 'VerifiedInstallation',\n",
      "       'PlaceVerification', 'NatureAudit', 'DateInspection', 'Intervenant',\n",
      "       'DateReport', 'PhaseReport', 'Producer', 'UserOfMachine',\n",
      "       'DateInMarketNewCondtion', 'DateInMarketEstablishment', 'TextFindings'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory path containing the PDF files\n",
    "directory_path = '/Users/rongwang/Documents/Dekra/reports-anuary2024/reports France/template1'\n",
    "\n",
    "df_France,failed_files =  process_directory(directory_path)\n",
    "\n",
    "print(df_France.columns)\n",
    "\n",
    "#textfindings_df = df_china[\"file_name\",\"phase of report\",\"text findings\"]\n",
    "#textfindings_df.to_json(\"text_findings.json\", orient='records', lines=True)\n",
    "# Assuming df is your DataFrame\n",
    "#textfindings_df.to_xml('text_findings.xml', index=False)\n",
    "\n",
    "#df_china.to_csv(tracy_file,index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "df_France.to_xml(\"./reports France/normal_files.xml\",index= False)\n",
    "df_France.to_csv(\"./reports France/normal_files.csv\",index= False,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 29\n"
     ]
    }
   ],
   "source": [
    "num_rows = df_France.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import docx\n",
    "\n",
    "# Load the document\n",
    "word_file_path = '/Users/rongwang/Documents/French/test.docx'\n",
    "\n",
    "# This function will extract data from the first table in the document.\n",
    "# You might need to adjust the logic to select the correct table.\n",
    "def extract_data_from_table(doc):\n",
    "    # Assuming that your data of interest is in the first table\n",
    "    table = doc.tables[0] \n",
    "\n",
    "    # Prepare a list to store each row as a dictionary\n",
    "    data = []\n",
    "\n",
    "    # Extract column headers from the first row of the table\n",
    "    headers = []\n",
    "    for cell in table.rows[0].cells:\n",
    "        headers.append(cell.text.strip())\n",
    "\n",
    "    # Extract the rest of the data\n",
    "    for row in table.rows[1:]:\n",
    "        row_data = {}\n",
    "        for idx, cell in enumerate(row.cells):\n",
    "            # Use the header from the corresponding column as the dictionary key\n",
    "            row_data[headers[idx]] = cell.text.strip()\n",
    "        data.append(row_data)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Extract the data\n",
    "table_data = extract_data_from_table(doc)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(table_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_path = 'E19546002301RK1.pdf'\n",
    "result = extract_data_from_pdf(pdf_path)\n",
    "print(result)\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "        summary_page = pdf.pages[2]  # Assuming the table of contents is on the fourth page\n",
    "        text = summary_page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace line breaks followed by a lowercase letter or certain characters with a space\n",
    "    cleaned_text = re.sub(r'\\n(?=[a-z\\-])', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage with your extracted text\n",
    "extracted_text = \"\"\"\n",
    "Nature of the \n",
    "audit Documentary assessment of work \n",
    "equipment \n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = clean_text(extracted_text)\n",
    "print(cleaned_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
